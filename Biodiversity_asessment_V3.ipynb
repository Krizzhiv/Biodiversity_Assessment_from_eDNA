{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOewrFqQdrG9HpBT1ApXhDH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Krizzhiv/Biodiversity_Assessment_from_eDNA/blob/main/Biodiversity_asessment_V3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_yfa3CgHq5uv"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# CELL 1: ENVIRONMENT SETUP\n",
        "# ==============================================================================\n",
        "# This cell installs all necessary external tools and Python libraries.\n",
        "\n",
        "# --- Install Command-Line Tools ---\n",
        "print(\"Installing VSEARCH...\")\n",
        "!apt-get -qq install vsearch > /dev/null\n",
        "print(\"VSEARCH installed.\")\n",
        "\n",
        "# --- Install Python Libraries ---\n",
        "print(\"Installing tensorflow...\")\n",
        "!pip install tensorflow > /dev/null\n",
        "print(\"tensorflow installed.\")\n",
        "\n",
        "print(\"Installing biopython...\")\n",
        "!pip install biopython > /dev/null\n",
        "print(\"biopython installed.\")\n",
        "\n",
        "# <<< ADDED HDBSCAN AND UMAP INSTALLATION HERE >>>\n",
        "print(\"Installing HDBSCAN & UMAP...\")\n",
        "!pip install hdbscan umap-learn > /dev/null\n",
        "print(\"HDBSCAN & UMAP installed.\")\n",
        "\n",
        "\n",
        "print(\"\\nEnvironment setup complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CELL 2: IMPORT LIBRARIES\n",
        "# ==============================================================================\n",
        "# This cell imports all the Python libraries we will use in our pipeline.\n",
        "\n",
        "import subprocess\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from Bio.Blast import NCBIWWW, NCBIXML\n",
        "from itertools import product\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import hdbscan\n",
        "import umap\n",
        "from scipy.spatial.distance import pdist\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "print(\"Libraries imported successfully.\")"
      ],
      "metadata": {
        "id": "p6EWJrHcrNof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CELL 3: PIPELINE FUNCTION DEFINITIONS\n",
        "# ==============================================================================\n",
        "# This cell contains all the core functions for the eDNA analysis pipeline.\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# FUNCTION 1: Load and Parse the FASTQ file\n",
        "# ------------------------------------------------------------------------------\n",
        "def load_and_parse_fastq(filename):\n",
        "    \"\"\"\n",
        "    Reads a FASTQ file and extracts the DNA sequences.\n",
        "\n",
        "    Args:\n",
        "        filename (str): The path to the FASTQ file.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of DNA sequence strings.\n",
        "    \"\"\"\n",
        "    print(f\"-> Loading and parsing '{filename}'...\")\n",
        "    dna_sequences = []\n",
        "    with open(filename, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "        for i in range(0, len(lines), 4):\n",
        "            sequence = lines[i+1].strip()\n",
        "            dna_sequences.append(sequence)\n",
        "    print(f\"   Done. Extracted {len(dna_sequences)} sequences.\")\n",
        "    return dna_sequences\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# FUNCTION 2: Run the VSEARCH Clustering\n",
        "# ------------------------------------------------------------------------------\n",
        "def run_vsearch_clustering(sequences, identity_threshold=0.97):\n",
        "    \"\"\"\n",
        "    Takes a list of DNA sequences and clusters them using VSEARCH.\n",
        "\n",
        "    Args:\n",
        "        sequences (list): A list of DNA sequence strings.\n",
        "        identity_threshold (float): The similarity threshold for clustering (0.0 to 1.0).\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the filenames of the centroid and cluster map files.\n",
        "               (e.g., ('centroids.fasta', 'clusters.uc'))\n",
        "    \"\"\"\n",
        "    # Define filenames\n",
        "    fasta_input_file = \"sequences_to_cluster.fasta\"\n",
        "    centroid_file = \"centroids.fasta\"\n",
        "    cluster_map_file = \"clusters.uc\"\n",
        "\n",
        "    # 1. Write sequences to a FASTA file for VSEARCH\n",
        "    print(f\"-> Writing {len(sequences)} sequences to '{fasta_input_file}'...\")\n",
        "    with open(fasta_input_file, 'w') as f:\n",
        "        for i, seq in enumerate(sequences):\n",
        "            f.write(f\">read_{i}\\n\")\n",
        "            f.write(f\"{seq}\\n\")\n",
        "\n",
        "    # 2. Construct and run the VSEARCH command\n",
        "    print(f\"-> Starting clustering with a {identity_threshold*100}% identity threshold...\")\n",
        "    command = [\n",
        "        \"vsearch\",\n",
        "        \"--cluster_fast\", fasta_input_file,\n",
        "        \"--id\", str(identity_threshold),\n",
        "        \"--centroids\", centroid_file,\n",
        "        \"--uc\", cluster_map_file\n",
        "    ]\n",
        "    # We use subprocess.run to execute the command\n",
        "    subprocess.run(command, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "\n",
        "    # 3. Count the number of clusters found\n",
        "    count_command = f\"grep -c '^>' {centroid_file}\"\n",
        "    result = subprocess.run(count_command, shell=True, capture_output=True, text=True)\n",
        "    num_clusters = int(result.stdout.strip())\n",
        "\n",
        "    print(f\"   Done. Found {num_clusters} unique clusters (OTUs).\")\n",
        "    return centroid_file, cluster_map_file\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# FUNCTION 3: Calculate the Abundance of Each Cluster\n",
        "# ------------------------------------------------------------------------------\n",
        "def calculate_abundance(cluster_map_file):\n",
        "    \"\"\"\n",
        "    Parses the VSEARCH cluster map file (.uc) to calculate OTU abundance.\n",
        "\n",
        "    Args:\n",
        "        cluster_map_file (str): The path to the .uc file from VSEARCH.\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: A DataFrame with 'cluster_num' and 'abundance', sorted.\n",
        "    \"\"\"\n",
        "    print(f\"-> Calculating abundance from '{cluster_map_file}'...\")\n",
        "    cluster_data = []\n",
        "    with open(cluster_map_file, 'r') as f:\n",
        "        for line in f:\n",
        "            if line.startswith('H') or line.startswith('C'):\n",
        "                parts = line.strip().split('\\t')\n",
        "                cluster_num = int(parts[1])\n",
        "                cluster_data.append([cluster_num])\n",
        "\n",
        "    df_clusters = pd.DataFrame(cluster_data, columns=['cluster_num'])\n",
        "    df_abundance = df_clusters['cluster_num'].value_counts().reset_index()\n",
        "    df_abundance.columns = ['cluster_num', 'abundance']\n",
        "    df_abundance = df_abundance.sort_values(by='abundance', ascending=False)\n",
        "    print(\"   Done.\")\n",
        "    return df_abundance\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# FUNCTION 4: Load Centroid Sequences into a Dictionary\n",
        "# ------------------------------------------------------------------------------\n",
        "def load_centroids(centroid_file):\n",
        "    \"\"\"\n",
        "    Loads the representative sequences from the centroids FASTA file.\n",
        "\n",
        "    Args:\n",
        "        centroid_file (str): The path to the centroids FASTA file.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary mapping read_id (str) to DNA sequence (str).\n",
        "    \"\"\"\n",
        "    print(f\"-> Loading representative sequences from '{centroid_file}'...\")\n",
        "    centroid_sequences = {}\n",
        "    with open(centroid_file, 'r') as f:\n",
        "        current_read_id = \"\"\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line.startswith('>'):\n",
        "                current_read_id = line[1:]\n",
        "                centroid_sequences[current_read_id] = \"\"\n",
        "            else:\n",
        "                centroid_sequences[current_read_id] += line\n",
        "    print(f\"   Done. Loaded {len(centroid_sequences)} sequences.\")\n",
        "    return centroid_sequences\n",
        "# ------------------------------------------------------------------------------\n",
        "# FUNCTION 5: Perform a BLAST Search for a single sequence\n",
        "# ------------------------------------------------------------------------------\n",
        "def get_blast_result(sequence):\n",
        "    \"\"\"\n",
        "    Performs an online BLAST search for a given DNA sequence and returns the top hit.\n",
        "\n",
        "    Args:\n",
        "        sequence (str): The DNA sequence to search for.\n",
        "\n",
        "    Returns:\n",
        "        str: A formatted string describing the top hit, or a \"Not Found\" message.\n",
        "    \"\"\"\n",
        "    # NCBIWWW.qblast can sometimes be slow or fail. A try/except block is good practice.\n",
        "    try:\n",
        "        result_handle = NCBIWWW.qblast(\"blastn\", \"nt\", sequence)\n",
        "        blast_record = NCBIXML.read(result_handle)\n",
        "\n",
        "        if len(blast_record.alignments) > 0:\n",
        "            top_alignment = blast_record.alignments[0]\n",
        "            species_name = top_alignment.title.split(',')[0] # Get a cleaner name\n",
        "            identity = (top_alignment.hsps[0].identities / top_alignment.hsps[0].align_length) * 100\n",
        "\n",
        "            return f\"{species_name} ({identity:.2f}% identity)\"\n",
        "        else:\n",
        "            return \"NO SIGNIFICANT MATCH FOUND (Novel Taxon)\"\n",
        "\n",
        "    except Exception as e:\n",
        "        # If the BLAST search fails for any reason (e.g., internet issue)\n",
        "        return f\"BLAST search failed: {e}\"\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# FUNCTION 6: Calculate Biodiversity Metrics\n",
        "# ------------------------------------------------------------------------------\n",
        "def calculate_diversity_metrics(df_abundance):\n",
        "  \"\"\"Calculate Shannon and Simpson diversity indices\"\"\"\n",
        "  total_reads = df_abundance['abundance'].sum()\n",
        "  proportions = df_abundance['abundance'] / total_reads\n",
        "\n",
        "  # Shannon Index\n",
        "  shannon = -sum(p * np.log(p) for p in proportions if p > 0)\n",
        "\n",
        "  # Simpson Index\n",
        "  simpson = sum(p**2 for p in proportions)\n",
        "\n",
        "  return {\n",
        "      'shannon_index': shannon,\n",
        "      'simpson_index': 1 - simpson,\n",
        "      'total_otus': len(df_abundance),\n",
        "      'effective_species': np.exp(shannon)  # Hill number\n",
        "  }\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# FUNCTION 7: Analyze Cluster Distribution\n",
        "# ------------------------------------------------------------------------------\n",
        "def analyze_cluster_distribution(df_abundance, threshold=0.01):\n",
        "    \"\"\"Identify rare vs dominant taxa\"\"\"\n",
        "    total = df_abundance['abundance'].sum()\n",
        "    df_abundance['relative_abundance'] = df_abundance['abundance'] / total\n",
        "\n",
        "    df_abundance['category'] = df_abundance['relative_abundance'].apply(\n",
        "        lambda x: 'Dominant' if x > 0.05 else 'Common' if x > threshold else 'Rare'\n",
        "    )\n",
        "\n",
        "    return df_abundance\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# FUNCTION 8: Generate K-mer Vocabulary\n",
        "# ------------------------------------------------------------------------------\n",
        "def generate_kmer_vocabulary(k=6):\n",
        "    \"\"\"Generate all possible k-mers for DNA\"\"\"\n",
        "    bases = ['A', 'T', 'G', 'C']\n",
        "    return [''.join(p) for p in product(bases, repeat=k)]\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# FUNCTION 9: Convert Sequence to K-mer Vector\n",
        "# ------------------------------------------------------------------------------\n",
        "def sequence_to_kmer_vector(sequence, k=6, stride=1):\n",
        "    \"\"\"Convert DNA sequence to k-mer frequency vector\"\"\"\n",
        "    kmer_vocab = generate_kmer_vocabulary(k)\n",
        "    kmer_to_idx = {kmer: idx for idx, kmer in enumerate(kmer_vocab)}\n",
        "\n",
        "    # Initialize frequency vector\n",
        "    vector = np.zeros(len(kmer_vocab))\n",
        "\n",
        "    # Count k-mers with overlapping window\n",
        "    for i in range(0, len(sequence) - k + 1, stride):\n",
        "        kmer = sequence[i:i+k]\n",
        "        if kmer in kmer_to_idx:\n",
        "            vector[kmer_to_idx[kmer]] += 1\n",
        "\n",
        "    # Normalize by sequence length\n",
        "    if vector.sum() > 0:\n",
        "        vector = vector / vector.sum()\n",
        "\n",
        "    return vector\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# CLASS 10: Variational Autoencoder (VAE) Model\n",
        "# ------------------------------------------------------------------------------\n",
        "class VAE(keras.Model):\n",
        "    def __init__(self, input_dim, latent_dim=32):\n",
        "        super(VAE, self).__init__()\n",
        "\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "\n",
        "        # Encoder\n",
        "        self.encoder = keras.Sequential([\n",
        "            layers.Dense(256, activation='relu', input_shape=(input_dim,)),\n",
        "            layers.Dropout(0.2),\n",
        "            layers.Dense(128, activation='relu'),\n",
        "            layers.Dropout(0.2),\n",
        "            layers.Dense(64, activation='relu')\n",
        "        ])\n",
        "\n",
        "        self.z_mean = layers.Dense(latent_dim)\n",
        "        self.z_log_var = layers.Dense(latent_dim)\n",
        "\n",
        "        # Decoder\n",
        "        self.decoder = keras.Sequential([\n",
        "            layers.Dense(64, activation='relu', input_shape=(latent_dim,)),\n",
        "            layers.Dropout(0.2),\n",
        "            layers.Dense(128, activation='relu'),\n",
        "            layers.Dropout(0.2),\n",
        "            layers.Dense(256, activation='relu'),\n",
        "            layers.Dense(input_dim)\n",
        "        ])\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        return self.z_mean(h), self.z_log_var(h)\n",
        "\n",
        "    def reparameterize(self, z_mean, z_log_var):\n",
        "        eps = tf.random.normal(shape=tf.shape(z_mean))\n",
        "        return eps * tf.exp(z_log_var * 0.5) + z_mean\n",
        "\n",
        "    def decode(self, z):\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def call(self, x):\n",
        "        z_mean, z_log_var = self.encode(x)\n",
        "        z = self.reparameterize(z_mean, z_log_var)\n",
        "        reconstructed = self.decode(z)\n",
        "\n",
        "        # Calculate loss\n",
        "        reconstruction_loss = tf.reduce_mean(\n",
        "            keras.losses.binary_crossentropy(x, reconstructed)\n",
        "        )\n",
        "\n",
        "        reconstruction_loss *= self.input_dim\n",
        "\n",
        "        kl_loss = -0.5 * tf.reduce_mean(\n",
        "            1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
        "        )\n",
        "\n",
        "        self.add_loss(reconstruction_loss + kl_loss)\n",
        "        return reconstructed\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# FUNCTION 11: Calculate Group Coherence\n",
        "# ------------------------------------------------------------------------------\n",
        "def calculate_group_coherence(group_embeddings):\n",
        "    \"\"\"Calculate how tightly clustered a group is\"\"\"\n",
        "    if len(group_embeddings) < 2:\n",
        "        return 1.0\n",
        "    distances = pdist(group_embeddings, metric='cosine')\n",
        "    return 1.0 - distances.mean() if len(distances) > 0 else 1.0\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# FUNCTION 12: Assess Novelty Score\n",
        "# ------------------------------------------------------------------------------\n",
        "def assess_novelty_score(embedding, all_embeddings, k=10):\n",
        "    \"\"\"Calculate novelty based on distance to nearest neighbors\"\"\"\n",
        "    # Ensure n_neighbors is not greater than the number of samples\n",
        "    n_samples = all_embeddings.shape[0]\n",
        "    if k >= n_samples:\n",
        "        k = n_samples - 1\n",
        "\n",
        "    nn = NearestNeighbors(n_neighbors=k)\n",
        "    nn.fit(all_embeddings)\n",
        "    distances, _ = nn.kneighbors([embedding])\n",
        "    novelty = distances[0].mean()\n",
        "    return novelty\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# FUNCTION 13: Generate Final Report\n",
        "# ------------------------------------------------------------------------------\n",
        "def generate_final_report():\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\" \"*20 + \"AI-DRIVEN eDNA BIODIVERSITY REPORT\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # 1. Overall Statistics\n",
        "    print(\"\\n📊 OVERALL STATISTICS\")\n",
        "    print(f\"  • Total sequences analyzed: {len(sequences)}\")\n",
        "    print(f\"  • Unique OTUs discovered: {len(df_abundance)}\")\n",
        "    print(f\"  • Shannon Diversity Index: {diversity_stats['shannon_index']:.3f}\")\n",
        "    print(f\"  • Effective Number of Species: {diversity_stats['effective_species']:.1f}\")\n",
        "\n",
        "    # 2. Ecological Group Summary\n",
        "    print(\"\\n🌊 ECOLOGICAL GROUPS (AI-Discovered)\")\n",
        "    print(f\"  • Total ecological groups: {n_clusters}\")\n",
        "    print(f\"  • Unclustered OTUs (potential novel): {n_noise}\")\n",
        "\n",
        "    if len(df_groups) > 0:\n",
        "        print(\"\\n  Top 5 Ecological Groups by Abundance:\")\n",
        "        for _, group in df_groups.nlargest(5, 'total_abundance').iterrows():\n",
        "            print(f\"    - Group {group['group_id']}: {group['num_otus']} OTUs, \"\n",
        "                  f\"{group['total_abundance']} reads, {group['classification']}\")\n",
        "\n",
        "    # 3. Novel Taxa Discovery\n",
        "    print(\"\\n🔬 NOVEL TAXA CANDIDATES\")\n",
        "    if novel_candidates:\n",
        "        print(f\"  • High-confidence novel taxa: {len(novel_candidates)}\")\n",
        "        for candidate in novel_candidates[:3]:\n",
        "            print(f\"    - OTU {candidate['otu_id']}: \"\n",
        "                  f\"Novelty score {candidate['novelty_score']:.3f}, \"\n",
        "                  f\"Abundance: {candidate['abundance']}\")\n",
        "    else:\n",
        "        print(\"  • No high-confidence novel taxa detected\")\n",
        "\n",
        "    # 4. Abundance Distribution\n",
        "    print(\"\\n📈 ABUNDANCE DISTRIBUTION\")\n",
        "    categories = df_abundance['category'].value_counts()\n",
        "    for cat, count in categories.items():\n",
        "        print(f\"  • {cat} taxa: {count} OTUs\")\n",
        "\n",
        "    # 5. Visualization\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "    # Plot 1: Abundance distribution\n",
        "    axes[0, 0].hist(np.log10(df_abundance['abundance'] + 1), bins=30, edgecolor='black')\n",
        "    axes[0, 0].set_xlabel('Log10(Abundance + 1)')\n",
        "    axes[0, 0].set_ylabel('Number of OTUs')\n",
        "    axes[0, 0].set_title('OTU Abundance Distribution')\n",
        "\n",
        "    # Plot 2: UMAP visualization\n",
        "    scatter = axes[0, 1].scatter(embedding_2d[:, 0], embedding_2d[:, 1],\n",
        "                                 c=cluster_labels, cmap='tab20', alpha=0.6)\n",
        "    axes[0, 1].set_xlabel('UMAP 1')\n",
        "    axes[0, 1].set_ylabel('UMAP 2')\n",
        "    axes[0, 1].set_title('Ecological Groups (AI Clustering)')\n",
        "\n",
        "    # Plot 3: Group sizes\n",
        "    if len(df_groups) > 0:\n",
        "        group_sizes = df_groups['total_abundance'].values[:10]\n",
        "        group_ids = [f\"G{g}\" for g in df_groups['group_id'].values[:10]]\n",
        "        axes[1, 0].bar(range(len(group_sizes)), group_sizes)\n",
        "        axes[1, 0].set_xticks(range(len(group_ids)))\n",
        "        axes[1, 0].set_xticklabels(group_ids)\n",
        "        axes[1, 0].set_xlabel('Ecological Group')\n",
        "        axes[1, 0].set_ylabel('Total Abundance')\n",
        "        axes[1, 0].set_title('Top 10 Ecological Groups')\n",
        "\n",
        "    # Plot 4: Novelty distribution\n",
        "    all_novelty_scores = [assess_novelty_score(e, embeddings) for e in embeddings[:100]]\n",
        "    axes[1, 1].hist(all_novelty_scores, bins=20, edgecolor='black', alpha=0.7)\n",
        "    axes[1, 1].axvline(np.percentile(all_novelty_scores, 90), color='red',\n",
        "                       linestyle='--', label='Novel threshold')\n",
        "    axes[1, 1].set_xlabel('Novelty Score')\n",
        "    axes[1, 1].set_ylabel('Number of OTUs')\n",
        "    axes[1, 1].set_title('Novelty Score Distribution')\n",
        "    axes[1, 1].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"Report generation complete. Results ready for interpretation.\")\n",
        "    print(\"=\"*70)"
      ],
      "metadata": {
        "id": "ptSJAQRnrOYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CELL 4: FINAL PIPELINE SCRIPT WITH TOP-N TAXONOMY ANALYSIS\n",
        "# ==============================================================================\n",
        "\n",
        "# --- Configuration ---\n",
        "input_file = \"sample_chunk_40k_to_80k.fastq\"\n",
        "num_top_otus_to_identify = 10\n",
        "\n",
        "# --- 1. Run the Core Pipeline ---\n",
        "print(\"--- [Step 1/3] Starting Core eDNA Analysis ---\")\n",
        "sequences = load_and_parse_fastq(input_file)\n",
        "centroid_filepath, cluster_map_filepath = run_vsearch_clustering(sequences)\n",
        "df_abundance = calculate_abundance(cluster_map_filepath)\n",
        "centroid_sequences = load_centroids(centroid_filepath)\n",
        "print(\"--- Core Analysis Complete ---\\n\")\n",
        "\n",
        "# --- 2. Get Centroid IDs for the Top N OTUs ---\n",
        "print(f\"--- [Step 2/3] Preparing to Identify Top {num_top_otus_to_identify} OTUs ---\")\n",
        "# We need to find the representative read ID for each of our top clusters\n",
        "# First, let's parse the cluster map (.uc) again to map cluster_num to centroid_id\n",
        "cluster_to_centroid = {}\n",
        "with open(cluster_map_filepath, 'r') as f:\n",
        "    for line in f:\n",
        "        if line.startswith('C'): # Centroid lines define the mapping\n",
        "            parts = line.strip().split('\\t')\n",
        "            cluster_num = int(parts[1])\n",
        "            centroid_id = parts[8]\n",
        "            cluster_to_centroid[cluster_num] = centroid_id\n",
        "\n",
        "# Add a new column to our abundance table with the centroid ID\n",
        "df_abundance['centroid_id'] = df_abundance['cluster_num'].map(cluster_to_centroid)\n",
        "print(\"--- Preparation Complete ---\\n\")\n",
        "\n",
        "# Run enhanced analysis\n",
        "df_abundance = analyze_cluster_distribution(df_abundance)\n",
        "diversity_stats = calculate_diversity_metrics(df_abundance)\n",
        "\n",
        "print(f\"Shannon Diversity: {diversity_stats['shannon_index']:.3f}\")\n",
        "print(f\"Effective Number of Species: {diversity_stats['effective_species']:.1f}\")"
      ],
      "metadata": {
        "id": "YWMNHyQlvUk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CELL 5: K-MER TOKENIZATION FOR DEEP LEARNING\n",
        "# ==============================================================================\n",
        "\n",
        "# Convert all centroid sequences to k-mer vectors\n",
        "print(\"Converting sequences to k-mer vectors...\")\n",
        "kmer_matrix = []\n",
        "sequence_ids = []\n",
        "\n",
        "for seq_id, sequence in centroid_sequences.items():\n",
        "    kmer_vector = sequence_to_kmer_vector(sequence, k=6, stride=2)\n",
        "    kmer_matrix.append(kmer_vector)\n",
        "    sequence_ids.append(seq_id)\n",
        "\n",
        "X = np.array(kmer_matrix)\n",
        "print(f\"Created matrix of shape: {X.shape}\")\n",
        "print(f\"  - {X.shape[0]} OTUs\")\n",
        "print(f\"  - {X.shape[1]} k-mer features\")\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "Jm5nFyX-2Oii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CELL 6: VARIATIONAL AUTOENCODER FOR REPRESENTATION LEARNING\n",
        "# ==============================================================================\n",
        "\n",
        "# Create and train VAE\n",
        "print(\"Building VAE model...\")\n",
        "vae = VAE(input_dim=X_scaled.shape[1], latent_dim=32)\n",
        "vae.compile(optimizer='adam')\n",
        "\n",
        "# Train the model\n",
        "print(\"Training VAE on your deep-sea eDNA data...\")\n",
        "history = vae.fit(\n",
        "    X_scaled, X_scaled,\n",
        "    epochs=100,\n",
        "    batch_size=32,\n",
        "    verbose=0,\n",
        "    validation_split=0.1\n",
        ")\n",
        "\n",
        "# Extract embeddings\n",
        "z_mean, _ = vae.encode(X_scaled)\n",
        "embeddings = z_mean.numpy()\n",
        "print(f\"Generated embeddings with shape: {embeddings.shape}\")"
      ],
      "metadata": {
        "id": "QP0AP9hNpZg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CELL 7: DISCOVER ECOLOGICAL GROUPS VIA HDBSCAN\n",
        "# ==============================================================================\n",
        "\n",
        "# First, reduce dimensions for visualization\n",
        "print(\"Reducing dimensions for visualization...\")\n",
        "reducer = umap.UMAP(n_components=2, random_state=42)\n",
        "embedding_2d = reducer.fit_transform(embeddings)\n",
        "\n",
        "# Perform hierarchical clustering\n",
        "print(\"Discovering ecological groups...\")\n",
        "clusterer = hdbscan.HDBSCAN(\n",
        "    min_cluster_size=5,\n",
        "    min_samples=3,\n",
        "    cluster_selection_epsilon=0.5\n",
        ")\n",
        "cluster_labels = clusterer.fit_predict(embeddings)\n",
        "\n",
        "# Analyze results\n",
        "n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
        "n_noise = list(cluster_labels).count(-1)\n",
        "\n",
        "print(f\"Found {n_clusters} ecological groups\")\n",
        "print(f\"Unclustered OTUs (potential novel taxa): {n_noise}\")\n",
        "\n",
        "# Create ecological group mapping\n",
        "ecological_groups = {}\n",
        "for i, label in enumerate(cluster_labels):\n",
        "    seq_id = sequence_ids[i]\n",
        "    ecological_groups[seq_id] = label\n",
        "\n",
        "# Add to abundance dataframe\n",
        "df_abundance['ecological_group'] = df_abundance['centroid_id'].map(ecological_groups)"
      ],
      "metadata": {
        "id": "Sce1XxIStkDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CELL 8: AI-DRIVEN TAXONOMY AND NOVEL TAXA DISCOVERY (Corrected)\n",
        "# ==============================================================================\n",
        "\n",
        "# === PASS 1: GATHER METRICS FOR ALL GROUPS ===\n",
        "# In this pass, we just collect the data without trying to classify it yet.\n",
        "print(\"--- [Pass 1/2] Analyzing discovered groups... ---\")\n",
        "group_analysis = []\n",
        "for group_id in df_abundance['ecological_group'].unique():\n",
        "    if group_id == -1:\n",
        "        continue\n",
        "\n",
        "    group_otus = df_abundance[df_abundance['ecological_group'] == group_id]\n",
        "    group_abundance = group_otus['abundance'].sum()\n",
        "    group_size = len(group_otus)\n",
        "\n",
        "    group_indices = [sequence_ids.index(otu_id) for otu_id in group_otus['centroid_id']]\n",
        "    group_embeddings = embeddings[group_indices]\n",
        "\n",
        "    coherence = calculate_group_coherence(group_embeddings)\n",
        "    mean_embedding = group_embeddings.mean(axis=0)\n",
        "    novelty = assess_novelty_score(mean_embedding, embeddings)\n",
        "\n",
        "    group_analysis.append({\n",
        "        'group_id': group_id,\n",
        "        'num_otus': group_size,\n",
        "        'total_abundance': group_abundance,\n",
        "        'coherence_score': coherence,\n",
        "        'novelty_score': novelty\n",
        "    })\n",
        "\n",
        "df_groups = pd.DataFrame(group_analysis)\n",
        "print(f\"   Done. Found {len(df_groups)} groups to analyze.\")\n",
        "\n",
        "# === PASS 2: CLASSIFY GROUPS BASED ON COMPLETE DATA ===\n",
        "# Now that we have all novelty scores, we can safely calculate percentiles.\n",
        "print(\"--- [Pass 2/2] Classifying groups and identifying novel candidates... ---\")\n",
        "if not df_groups.empty:\n",
        "    novelty_threshold = df_groups['novelty_score'].quantile(0.75)\n",
        "    df_groups['classification'] = df_groups['novelty_score'].apply(\n",
        "        lambda x: 'Novel Group' if x > novelty_threshold else 'Known Pattern'\n",
        "    )\n",
        "\n",
        "# For unclustered OTUs (group -1), assess individually\n",
        "unclustered = df_abundance[df_abundance['ecological_group'] == -1]\n",
        "novel_candidates = []\n",
        "\n",
        "all_novelty_scores = [assess_novelty_score(e, embeddings) for e in embeddings]\n",
        "novelty_candidate_threshold = np.percentile(all_novelty_scores, 90)\n",
        "\n",
        "for _, otu in unclustered.iterrows():\n",
        "    idx = sequence_ids.index(otu['centroid_id'])\n",
        "    novelty = all_novelty_scores[idx]\n",
        "\n",
        "    if novelty > novelty_candidate_threshold:\n",
        "        novel_candidates.append({\n",
        "            'otu_id': otu['cluster_num'],\n",
        "            'abundance': otu['abundance'],\n",
        "            'novelty_score': novelty,\n",
        "            'status': 'High Priority Novel Taxon Candidate'\n",
        "        })\n",
        "print(f\"   Done. Identified {len(novel_candidates)} high-priority novel candidates.\")\n",
        "\n",
        "# === SELECTIVE BLAST VALIDATION ===\n",
        "print(\"\\n=== Selective Validation with BLAST ===\")\n",
        "print(\"(Validating top 3 groups and top 3 novel candidates)\")\n",
        "\n",
        "if not df_groups.empty:\n",
        "    top_groups = df_groups.nlargest(3, 'total_abundance')\n",
        "    for _, group in top_groups.iterrows():\n",
        "        group_otus = df_abundance[df_abundance['ecological_group'] == group['group_id']]\n",
        "        representative_otu = group_otus.nlargest(1, 'abundance').iloc[0]\n",
        "        representative_id = representative_otu['centroid_id']\n",
        "        sequence = centroid_sequences[representative_id]\n",
        "\n",
        "        print(f\"\\nGroup #{group['group_id']} (Abundance: {group['total_abundance']}):\")\n",
        "        blast_result = get_blast_result(sequence[:500])\n",
        "        print(f\"  Representative (OTU #{representative_otu['cluster_num']}): {blast_result}\")\n",
        "        print(f\"  AI Classification: {group['classification']}\")\n",
        "else:\n",
        "    print(\"\\nNo groups were found to validate.\")\n",
        "\n",
        "if novel_candidates:\n",
        "    top_novel = sorted(novel_candidates, key=lambda x: x['novelty_score'], reverse=True)[:3]\n",
        "    for candidate in top_novel:\n",
        "        otu_id_to_find = df_abundance[df_abundance['cluster_num'] == candidate['otu_id']].iloc[0]['centroid_id']\n",
        "        sequence = centroid_sequences[otu_id_to_find]\n",
        "\n",
        "        print(f\"\\nHigh Priority Candidate (OTU #{candidate['otu_id']}):\")\n",
        "        blast_result = get_blast_result(sequence[:500])\n",
        "        print(f\"  Novelty Score: {candidate['novelty_score']:.4f}\")\n",
        "        print(f\"  BLAST Result: {blast_result}\")\n",
        "else:\n",
        "    print(\"\\nNo high-priority novel candidates found.\")"
      ],
      "metadata": {
        "id": "6pZd2OVPxdoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# CELL 9: GENERATE FINAL BIODIVERSITY REPORT\n",
        "# ==============================================================================\n",
        "\n",
        "# Generate the report\n",
        "generate_final_report()"
      ],
      "metadata": {
        "id": "GGBR2uvj2-3K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}